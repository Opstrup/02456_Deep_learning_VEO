{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import utils\n",
    "from datetime import datetime\n",
    "from BatchLoader2 import BatchLoader2 as BatchLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up given parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEIGHT, WIDTH = 128, 128\n",
    "NUMBER_CHANNELS = 1\n",
    "NUMBER_OUTPUT_CLASS = 2\n",
    "NUMBER_OUTPUT_LOC = 2\n",
    "GPU_OPTS = tf.GPUOptions(per_process_gpu_memory_fraction=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyperParameter setup for experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-4\n",
    "BATCH_SIZE = 32\n",
    "MAX_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the BatchLoader and show a sample image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchHandler = BatchLoader(dataset_folder_path=\"synthetic2_128x128\", batch_size=BATCH_SIZE, keep_green=True, expand_dims=False)\n",
    "x_batch, y_batch, y_loc_batch = batchHandler.next_training_data()\n",
    "batchHandler.reset_training_data()\n",
    "plt.imshow(np.squeeze(x_batch[0]),cmap='gray')\n",
    "print(y_batch[0])\n",
    "print(y_loc_batch[0]*128.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building different types of networks for the experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build a building block for features, if poolsize and stride is the same, then the poolings are not overlapping\n",
    "def conv_pool_drop(x,filters,kernel,pool,stride,keep_prob=1.0, counter=1, padding='same',use_avg=True,trace=False):\n",
    "    with tf.variable_scope('conv'+str(counter)):\n",
    "        lc1 = tf.layers.conv2d(x, filters, kernel, (1,1), padding=padding) #Do not downsample with cross-correlation\n",
    "        if(trace is True):\n",
    "            print(lc1)\n",
    "    with tf.variable_scope('maxpool'+str(counter)):\n",
    "        lp1 = tf.layers.max_pooling2d(lc1,pool,stride) #Pool it to downsample\n",
    "        if(trace is True):\n",
    "            print(lp1)\n",
    "    if(use_avg is True):\n",
    "        with tf.variable_scope('avgpool'+str(counter)): #We also introduce average pooling as maxpooling is rather agressive\n",
    "            lap1 = tf.layers.average_pooling2d(lc1,pool,stride)\n",
    "            if(trace is True):\n",
    "                print(lap1)\n",
    "        with tf.variable_scope('mergepool'+str(counter)):\n",
    "            m1 = tf.concat([lp1,lap1],axis=3)\n",
    "            if(trace is True):\n",
    "                print(m1)\n",
    "    else:\n",
    "        m1 = lp1\n",
    "    with tf.variable_scope('dropout'+str(counter)):\n",
    "        ld1 = tf.layers.dropout(m1,rate=1.0-keep_prob)\n",
    "        if(trace is True):\n",
    "            print(ld1)\n",
    "    return lc1,m1,ld1\n",
    "\n",
    "def our_network(x_pl,hyper,trace=False,use_avg=True):\n",
    "    #Our architecture\n",
    "    x = x_pl\n",
    "    #Architecture hyperparams\n",
    "    blocknumbers = hyper[0] #3\n",
    "    padding = hyper[1] #'same'\n",
    "    filters = hyper[2] #[8,3,3]\n",
    "    kernel_size = hyper[3] #[(7,7),(5,5),(3,3)]\n",
    "    stride = hyper[4] #[(3,3),(3,3),(5,5)]\n",
    "    pool_size = hyper[5] #[(5,5),(5,5),(3,3)]\n",
    "    denseNum = hyper[6] #512\n",
    "    denseNumReg = hyper[7] #512\n",
    "    keep_prob = hyper[8]#tf.placeholder(\"float\",name=\"KeepProbabilityPool\")\n",
    "    keep_prob2 = hyper[9]#tf.placeholder(\"float\",name=\"KeepProbabilityDense\")\n",
    "    lc = []\n",
    "    m = []\n",
    "    ld = []\n",
    "    #Need to initialize first layer manually, such that the loop can do it automatically later on\n",
    "    lc1,m1,ld1 = conv_pool_drop(x,filters[0],kernel_size[0],pool_size[0],stride[0],keep_prob=keep_prob,counter=1,trace=trace,use_avg=use_avg)\n",
    "    lc.append(lc1)\n",
    "    m.append(m1)\n",
    "    ld.append(ld1)\n",
    "    for i in range(2,blocknumbers+1):\n",
    "        i2 = min(i-1,len(filters)-1) ##If there are more layers, use the settings for the last one\n",
    "        lc1,m1,ld1 = conv_pool_drop(ld[-1],filters[i2],kernel_size[i2],pool_size[i2],stride[i2],keep_prob=keep_prob,counter=i,trace=trace,use_avg=use_avg)\n",
    "        lc.append(lc1)\n",
    "        m.append(m1)\n",
    "        ld.append(ld1)\n",
    "    #Flatten and concatenate to evaluate the different features separately\n",
    "    flattened = [] #Contains flattened feature maps on different levels\n",
    "    with tf.variable_scope(\"Flatten\"):\n",
    "        for i in range(0,len(ld)):\n",
    "            flattened.append(tf.layers.flatten(ld[i]))\n",
    "            if(trace is True):\n",
    "                print(flattened[i])\n",
    "    with tf.variable_scope(\"Concatenate\"):\n",
    "        concat = tf.concat(flattened,axis=1)\n",
    "        if(trace is True):\n",
    "            print(concat)\n",
    "    #Dense layers with dropout\n",
    "    with tf.variable_scope(\"DenseInterpreterClass\"):\n",
    "        hiddenDense = tf.layers.dense(concat,denseNum,activation=tf.nn.relu,name=\"hidden_DenseClass\")\n",
    "        if(trace is True):\n",
    "            print(hiddenDense)\n",
    "        doDense = tf.layers.dropout(hiddenDense,rate=1.0-keep_prob2)\n",
    "        if(trace is True):\n",
    "            print(doDense)\n",
    "    with tf.variable_scope(\"DenseInterpreterRegress\"):\n",
    "        hiddenDenseReg = tf.layers.dense(concat,denseNumReg,activation=tf.nn.relu,name=\"hidden_DenseRegress\")\n",
    "        if(trace is True):\n",
    "            print(hiddenDenseReg)\n",
    "        doDenseReg = tf.layers.dropout(hiddenDenseReg,rate=1.0-keep_prob2)\n",
    "        if(trace is True):\n",
    "            print(doDenseReg)\n",
    "        \n",
    "    #Output layers\n",
    "    with tf.variable_scope(\"Predictions\"):\n",
    "        l_class = tf.layers.dense(doDense,NUMBER_OUTPUT_CLASS,activation=tf.nn.softmax,name=\"ClassificationGuess\")\n",
    "        if(trace is True):\n",
    "            print(l_class)\n",
    "        l_loc = tf.layers.dense(doDenseReg,NUMBER_OUTPUT_LOC,activation=tf.nn.relu,name=\"RegressionGuess\")\n",
    "        if(trace is True):\n",
    "            print(l_loc)\n",
    "    return l_class,l_loc\n",
    "\n",
    "def build_networks(network_type,hyper,placeholders,trace=False):\n",
    "    #Input type is common in all\n",
    "    x_pl = placeholders[0]\n",
    "    y_pl = placeholders[1]\n",
    "    y_loc = placeholders[2]\n",
    "    #Set up different network architecture\n",
    "    if(network_type is 1):\n",
    "        hyper.append(placeholders[3]) #Keep probability1\n",
    "        hyper.append(placeholders[4]) #Keep probability2\n",
    "        l_class, l_loc = our_network(x_pl,hyper,trace=trace,use_avg=True)\n",
    "    elif (network_type is 2):\n",
    "        # Do the same as before just without the averagepooling\n",
    "        hyper.append(placeholders[3])\n",
    "        hyper.append(placeholders[4])\n",
    "        l_class, l_loc = our_network(x_pl,hyper,trace=trace,use_avg=False)\n",
    "    else:\n",
    "        #Network architecture provided by TAX\n",
    "        l = x_pl\n",
    "        with tf.variable_scope('conv2d'):\n",
    "            l = tf.layers.conv2d(l, 16*4, [3, 3], 4, padding='VALID', name='conv1')\n",
    "            if(trace is True):\n",
    "                print(l)\n",
    "            l = tf.layers.max_pooling2d(l, [2, 2], 2, name='pool1')\n",
    "            if(trace is True):\n",
    "                print(l)\n",
    "            l = tf.layers.conv2d(l, 16*12, [3, 3], padding='VALID', name='conv2')\n",
    "            if(trace is True):\n",
    "                print(l)\n",
    "            l = tf.layers.max_pooling2d(l, [2, 2], 2, name='pool2')\n",
    "            if(trace is True):\n",
    "                print(l)\n",
    "            l = tf.layers.conv2d(l, 16*24, [3, 3], padding='VALID', name='conv3')\n",
    "            if(trace is True):\n",
    "                print(l)\n",
    "            l = tf.layers.conv2d(l, 16*32, [3, 3], padding='VALID', name='conv4')\n",
    "            if(trace is True):\n",
    "                print(l)\n",
    "        with tf.variable_scope('flatten'):\n",
    "            l = tf.layers.flatten(l)\n",
    "            if(trace is True):\n",
    "                print(l)\n",
    "        with tf.variable_scope('fully_connected'):\n",
    "            l_class= tf.layers.dense(l,2, activation=tf.nn.softmax, name=\"fc_last\")\n",
    "            if(trace is True):\n",
    "                print(l)\n",
    "            #Added for regression; not present in provided architecture\n",
    "            l_loc = tf.layers.dense(l,2, activation=tf.nn.relu, name=\"fc_loc\")\n",
    "    print('Model consits of ', utils.num_params(), 'trainable parameters.')\n",
    "    \n",
    "    #Loss, training, etc\n",
    "    with tf.variable_scope('loss'):\n",
    "        with tf.variable_scope('loss_class'):\n",
    "            # computing cross entropy per sample for classification\n",
    "            cross_entropy = -tf.reduce_sum(y_pl * tf.log(l_class+1e-8), reduction_indices=[1])\n",
    "\n",
    "            # averaging over samples\n",
    "            cross_entropy = tf.reduce_mean(cross_entropy)\n",
    "            tf.summary.scalar('cross_entropy',cross_entropy)\n",
    "\n",
    "        with tf.variable_scope('loss_local'):\n",
    "            #Get one-hot encoding for representing if the ball is present in the image\n",
    "            ball_present = tf.cast(tf.equal(tf.argmax(y_pl,axis=1),tf.cast(1,tf.int64)),tf.float32)\n",
    "            ball_present = tf.expand_dims(ball_present,1)\n",
    "            #Compute mean squared error\n",
    "            mse = tf.losses.mean_squared_error(y_loc,l_loc,weights=ball_present)#Ignore cases when ball is not present in image\n",
    "            tf.summary.scalar('mean_sqared_error',mse)\n",
    "        loss = cross_entropy+mse\n",
    "        tf.summary.scalar('combined_loss',loss)\n",
    "        reg_scale = 0.0005\n",
    "        regularize = tf.contrib.layers.l2_regularizer(reg_scale)\n",
    "        params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "        reg_term = sum([regularize(param) for param in params])\n",
    "        loss += reg_term\n",
    "        tf.summary.scalar('reg_combined_loss',loss)\n",
    "    with tf.variable_scope('training'):\n",
    "        # defining our optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE)\n",
    "\n",
    "        # applying the gradients\n",
    "        train_op = optimizer.minimize(loss)\n",
    "\n",
    "\n",
    "    with tf.variable_scope('performance_class'):\n",
    "        # making a one-hot encoded vector of correct (1) and incorrect (0) predictions\n",
    "        correct_prediction = tf.equal(tf.argmax(l_class, axis=1), tf.argmax(y_pl, axis=1))\n",
    "\n",
    "        # averaging the one-hot encoded vector\n",
    "        accuracy_class = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        tf.summary.scalar('class_acc',accuracy_class)\n",
    "    \n",
    "    with tf.variable_scope('performance_local'):\n",
    "        #Calculate eucledian distance between the predicted coordinates and the labelled ones\n",
    "        ball_present = tf.cast(tf.equal(tf.argmax(y_pl,axis=1),tf.cast(1,tf.int64)),tf.float32)\n",
    "        avgdist = ball_present*tf.norm((l_loc-y_loc)*128,axis=1,keep_dims=False,ord=2)#convert to pixel values\n",
    "        avgdist = tf.reduce_sum(avgdist)/tf.maximum(1.0,tf.reduce_sum(ball_present)) #ignore predictions when ball is not present, safeguard if there is no ball in any of the pictures\n",
    "        tf.summary.scalar('avg_pixel_dev',avgdist)\n",
    "    merged_sum = tf.summary.merge_all()\n",
    "    t = datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "    train_writer = tf.summary.FileWriter('./summaries/train/'+t,tf.get_default_graph())\n",
    "    valid_writer = tf.summary.FileWriter('./summaries/valid/'+t,tf.get_default_graph())\n",
    "    return l_class,train_op,accuracy_class,cross_entropy,l_loc,avgdist,loss,mse,merged_sum,train_writer,valid_writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reset graph before building\n",
    "tf.reset_default_graph()\n",
    "with tf.name_scope('input'):\n",
    "    x_pl = tf.placeholder(tf.float32, [None, HEIGHT, WIDTH, NUMBER_CHANNELS], name='ImagePlaceholder')\n",
    "with tf.name_scope('output'):\n",
    "    y_pl = tf.placeholder(tf.float64, [None, NUMBER_OUTPUT_CLASS], name='Classification')\n",
    "    y_pl = tf.cast(y_pl, tf.float32)\n",
    "    #We are predicting pixel locations, in percentage of the pixel width: 1.0,1.0 corresponds to 128,128\n",
    "    y_loc = tf.placeholder(tf.float64, [None, NUMBER_OUTPUT_LOC], name='Regression')\n",
    "    y_loc = tf.cast(y_loc, tf.float32)\n",
    "with tf.name_scope('keepProbs'):\n",
    "    keep_prob = tf.placeholder(\"float\",name=\"KeepProbabilityPool\")\n",
    "    keep_prob2 = tf.placeholder(\"float\",name=\"KeepProbabilityDense\")\n",
    "#Setting up placeholders and hyperparameters\n",
    "hype = [3,'same',[8,3,3],[(7,7),(5,5),(3,3)],[(3,3),(3,3),(5,5)],[(5,5),(5,5),(3,3)],64,256]# hyper parameters\n",
    "l_c,train_op,accuracy_class,cross_entropy,l_loc,avgdist,loss,mse,merged_sum,tw,vw = build_networks(1,hype,[x_pl,y_pl,y_loc,keep_prob,keep_prob2],trace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session(config=tf.ConfigProto(gpu_options=GPU_OPTS)) as sess:\n",
    "    tmp_def = utils.rename_nodes(sess.graph_def, lambda s:\"/\".join(s.split('_',1)))\n",
    "    utils.show_graph(tmp_def)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchHandler = BatchLoader(dataset_folder_path=\"synthetic2_128x128\", batch_size=BATCH_SIZE, keep_green=True, expand_dims=False)\n",
    "x_batch, y_batch, y_loc_batch = batchHandler.next_training_data()\n",
    "batchHandler.reset_training_data()\n",
    "with tf.Session(config=tf.ConfigProto(gpu_options=GPU_OPTS)) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    y_pred_c,y_pred_loc = sess.run(fetches=[l_c,l_loc], feed_dict={x_pl: x_batch,y_pl:y_batch, keep_prob: 0.8, keep_prob2: 0.6})\n",
    "\n",
    "assert y_pred_c.shape == y_batch.shape, \"ERROR the output shape is not as expected!\" \\\n",
    "        + \" Output shape should be \" + str(l_c.shape) + ' but was ' + str(y_pred_c.shape)\n",
    "assert y_pred_loc.shape == y_loc_batch.shape, \"ERROR the output shape is not as expected!\" \\\n",
    "        + \" Output shape should be \" + str(l_loc.shape) + ' but was ' + str(y_pred_loc.shape)\n",
    "\n",
    "print('Forward pass successful!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Begin training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_loss_c,valid_loss_r,valid_loss, valid_accuracy_c,valid_accuracy_r = [], [],[],[],[]\n",
    "train_loss_c,train_loss_r,train_loss, train_accuracy_c,train_accuracy_r = [], [],[],[],[]\n",
    "saver = tf.train.Saver()\n",
    "validate_every_epoch = 0.01*10\n",
    "nextvalid = 0.0+validate_every_epoch\n",
    "currentstep = 0 #keeping track of global steps\n",
    "currentvstep = 0\n",
    "with tf.Session(config=tf.ConfigProto(gpu_options=GPU_OPTS)) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print('Begin training loop')\n",
    "    try:\n",
    "        _train_loss_c, _train_accuracy_c = [], []\n",
    "        _train_loss_r, _train_accuracy_r = [], []\n",
    "        _train_loss = []\n",
    "        while batchHandler.getEpoch() < MAX_EPOCHS: \n",
    "            currentstep = currentstep+1\n",
    "            ## Run train op\n",
    "            x_batch, y_batch, y_batch_loc = batchHandler.next_training_data()\n",
    "            fetches_train = [train_op, cross_entropy, accuracy_class,avgdist,loss,mse,merged_sum]\n",
    "            feed_dict_train = {x_pl: x_batch, y_pl: y_batch, y_loc:y_batch_loc, keep_prob: 0.8, keep_prob2: 0.6}\n",
    "            _, _loss_c, _acc_c,_acc_r,_loss,_loss_r,summary = sess.run(fetches_train, feed_dict_train)\n",
    "            _train_loss_c.append(_loss_c)\n",
    "            _train_accuracy_c.append(_acc_c)\n",
    "            _train_loss_r.append(_loss_r)\n",
    "            _train_accuracy_r.append(_acc_r)\n",
    "            _train_loss.append(_loss)\n",
    "            #record training summary to directory\n",
    "            tw.add_summary(summary,currentstep)\n",
    "            ## It is time to validate\n",
    "            if batchHandler.getEpoch() % 1.0 == 0 or batchHandler.getEpoch() > nextvalid:\n",
    "                print(\"Starting validation \\n\")\n",
    "                nextvalid += validate_every_epoch\n",
    "                #Record training statistics\n",
    "                train_loss.append(np.mean(_train_loss))\n",
    "                train_loss_r.append(np.mean(_train_loss_r))\n",
    "                train_loss_c.append(np.mean(_train_loss_c))\n",
    "                train_accuracy_c.append(np.mean(_train_accuracy_c))\n",
    "                train_accuracy_r.append(np.mean(_train_accuracy_r))\n",
    "\n",
    "                #Reset arrays until next validation\n",
    "                _train_loss_c, _train_accuracy_c = [], []\n",
    "                _train_loss_r, _train_accuracy_r = [], []\n",
    "                _train_loss = []\n",
    "                #Begin validation\n",
    "\n",
    "                fetches_valid = [cross_entropy, accuracy_class,mse,avgdist,loss,merged_sum]\n",
    "                _valid_loss, _valid_accuracy_c,_valid_loss_c,_valid_loss_r, _valid_accuracy_r = [], [],[],[],[]\n",
    "                while(batchHandler.getValid() != 1.0):\n",
    "                    currentvstep = currentvstep+1\n",
    "                    valid_x, valid_y, y_valid_loc = batchHandler.next_validation_data()\n",
    "                    feed_dict_valid = {x_pl: valid_x, y_pl: valid_y, y_loc:y_valid_loc, keep_prob: 1.0, keep_prob2: 1.0}\n",
    "                    _loss_c, _acc_c,_loss_r,_acc_r,_loss, summary = sess.run(fetches_valid, feed_dict_valid)\n",
    "                    _valid_loss.append(_loss)\n",
    "                    _valid_loss_c.append(_loss_c)\n",
    "                    _valid_loss_r.append(_loss_r)\n",
    "                    _valid_accuracy_c.append(_acc_c)\n",
    "                    _valid_accuracy_r.append(_acc_r)\n",
    "                    vw.add_summary(summary,currentvstep)\n",
    "                valid_loss.append(np.mean(_valid_loss))\n",
    "                valid_loss_c.append(np.mean(_valid_loss_c))\n",
    "                valid_loss_r.append(np.mean(_valid_loss_r))\n",
    "                valid_accuracy_c.append(np.mean(_valid_accuracy_c))\n",
    "                valid_accuracy_r.append(np.mean(_valid_accuracy_r))\n",
    "                summary = tf.Summary(value=[tf.Summary.Value(tag=\"loss_total_at_epoch\", simple_value=valid_loss[-1]),tf.Summary.Value(tag=\"loss_class_at_epoch\", simple_value=valid_loss_c[-1]),tf.Summary.Value(tag=\"loss_regress_at_epoch\", simple_value=valid_loss_r[-1]),tf.Summary.Value(tag=\"class_acc_at_epoch\", simple_value=valid_acc_c[-1]),tf.Summary.Value(tag=\"average_pixel_deviation\", simple_value=valid_acc_r[-1]),\n",
    "                    tf.Summary.Value(tag=\"train_loss_total_at_epoch\", simple_value=train_loss[-1]),tf.Summary.Value(tag=\"train_loss_class_at_epoch\", simple_value=train_loss_c[-1]),tf.Summary.Value(tag=\"train_loss_regress_at_epoch\", simple_value=train_loss_r[-1]),tf.Summary.Value(tag=\"train_class_acc_at_epoch\", simple_value=train_acc_c[-1]),tf.Summary.Value(tag=\"train_average_pixel_deviation\", simple_value=train_acc_r[-1]),])\n",
    "                vw.add_summary(summary,batchHandler.getEpoch())\n",
    "                print(\"Epoch {} : Train Loss {:6.3f} (classification); {:6.3f} (regression); {:6.3f} (total), Train acc {:6.3f} (classification);{:6.3f} (regression),  Valid loss {:6.3f} (classification); {:6.3f} (regression); {:6.3f} (total),  Valid acc {:6.3f} (classification);{:6.3f} (regression)\".format(\n",
    "                        batchHandler.getEpoch(), train_loss_c[-1], train_loss_r[-1], train_loss[-1], train_accuracy_c[-1], train_accuracy_r[-1], valid_loss_c[-1], valid_loss_r[-1], valid_loss[-1], valid_accuracy_c[-1], valid_accuracy_r[-1]))\n",
    "                batchHandler.reset_validation()\n",
    "                save_path = saver.save(sess, \"./checkpoints/ourballnoball\"+str(batchHandler.getEpoch())+\".ckpt\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
